{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd071f835d09f2dc7658b8bfea70337fd38a098aa1e53e53fe645aaa17deb87e84b",
   "display_name": "Python 3.7.9 64-bit ('E01': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from drivers.loaders.imdb import IMDB\n",
    "from drivers.loaders.newsspace200 import Newsspace200\n",
    "from drivers.loaders.sentimentMD import SentimentMD\n",
    "from drivers.loaders.sentiment140 import Sentiment140\n",
    "\n",
    "from drivers.tokenizers.word_piece_vocab import WordPieceVocab\n",
    "from drivers.tokenizers.word_level_vocab import WordLevelVocab\n",
    "from drivers.tokenizers.unigram_vocab import UnigramVocab\n",
    "from drivers.tokenizers.bpe_vocab import BPEVocab\n",
    "\n",
    "from drivers.models.simple import Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PATH_VOCABS = \"vocabs/\"\n",
    "PATH_ENCODES = \"encodes/\"\n",
    "MAX_PEDDING_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(model, data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        output = model.encode(item)\n",
    "        result.append(output.ids)\n",
    "    return result"
   ]
  },
  {
   "source": [
    "# Data loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dbs = [\n",
    "    { \"db\" : SentimentMD(\"data\") },\n",
    "    { \"db\" : Sentiment140(\"data\") },\n",
    "    { \"db\" : Newsspace200(\"data\") },\n",
    "    { \"db\" : IMDB(\"data\") }\n",
    "]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "# Train tokenizers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dbs:\n",
    "    item[\"vocabs\"] = [\n",
    "        WordPieceVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        WordLevelVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        UnigramVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        BPEVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE)\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabs/word_piece_IMDB.json\nLOADED: IMDB\nvocabs/word_level_IMDB.json\nLOADED: IMDB\nvocabs/unigram_IMDB.json\nLOADED: IMDB\nvocabs/bpe_IMDB.json\nLOADED: IMDB\n"
     ]
    }
   ],
   "source": [
    "for item in dbs:\n",
    "    for vocab in item[\"vocabs\"]:\n",
    "        file_name_vocabs = PATH_VOCABS + vocab.name + \"_\" + item[\"db\"].name + \".json\"\n",
    "        print(file_name_vocabs)\n",
    "        if os.path.isfile(file_name_vocabs) == False:\n",
    "            vocab.train()\n",
    "            print(\"TRAINED:\", item[\"db\"].name)\n",
    "            vocab.save(file_name_vocabs)\n",
    "            print(\"SAVED:\", item[\"db\"].name)\n",
    "        else:\n",
    "            vocab.load(file_name_vocabs)\n",
    "            print(\"LOADED:\", item[\"db\"].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENCODED (LOADED): encodes/word_piece_IMDB_train.json encodes/word_piece_IMDB_test.json word_piece IMDB\n",
      "PADDED TRAIN AND TEST:  694 word_piece IMDB\n",
      "ENCODED (LOADED): encodes/word_level_IMDB_train.json encodes/word_level_IMDB_test.json word_level IMDB\n",
      "PADDED TRAIN AND TEST:  412 word_level IMDB\n",
      "ENCODED (LOADED): encodes/unigram_IMDB_train.json encodes/unigram_IMDB_test.json unigram IMDB\n",
      "PADDED TRAIN AND TEST:  720 unigram IMDB\n",
      "ENCODED (LOADED): encodes/bpe_IMDB_train.json encodes/bpe_IMDB_test.json bpe IMDB\n",
      "PADDED TRAIN AND TEST:  650 bpe IMDB\n"
     ]
    }
   ],
   "source": [
    "for item in dbs:\n",
    "    item[\"encodes\"] = []\n",
    "    for vocab in item[\"vocabs\"]:\n",
    "        file_name_encodes_train = PATH_ENCODES + vocab.name + \"_\" + item[\"db\"].name + \"_train.json\"\n",
    "        file_name_encodes_test = PATH_ENCODES + vocab.name + \"_\" + item[\"db\"].name + \"_test.json\"\n",
    "        item[\"encodes\"].append({\n",
    "            \"train\" : pd.DataFrame(),\n",
    "            \"test\" : pd.DataFrame(),\n",
    "        })\n",
    "        if os.path.isfile(file_name_encodes_train) == False or os.path.isfile(file_name_encodes_test) == False :\n",
    "            item[\"encodes\"][-1][\"train\"][\"X\"] = encode(vocab, item[\"db\"].get_train()[\"text\"].values)\n",
    "            item[\"encodes\"][-1][\"train\"][\"y\"] = item[\"db\"].get_train()[\"label\"].values\n",
    "            item[\"encodes\"][-1][\"train\"].to_json(file_name_encodes_train, orient=\"records\", lines=True)\n",
    "\n",
    "            item[\"encodes\"][-1][\"test\"][\"X\"] = encode(vocab, item[\"db\"].get_test()[\"text\"].values)\n",
    "            item[\"encodes\"][-1][\"test\"][\"y\"] = item[\"db\"].get_test()[\"label\"].values\n",
    "            item[\"encodes\"][-1][\"test\"].to_json(file_name_encodes_test, orient=\"records\", lines=True)\n",
    "            print(\"ENCODED (CREATED AND LOADED):\", \n",
    "                  file_name_encodes_train, \n",
    "                  file_name_encodes_test,\n",
    "                  vocab.name, \n",
    "                  item[\"db\"].name)\n",
    "        else:                        \n",
    "            item[\"encodes\"][-1][\"train\"] = pd.read_json(file_name_encodes_train, orient=\"records\", lines=True)\n",
    "            item[\"encodes\"][-1][\"test\"] = pd.read_json(file_name_encodes_test, orient=\"records\", lines=True)\n",
    "            print(\"ENCODED (LOADED):\", \n",
    "                  file_name_encodes_train, \n",
    "                  file_name_encodes_test, \n",
    "                  vocab.name, \n",
    "                  item[\"db\"].name)\n",
    "\n",
    "        tmp_sorted = list(item[\"encodes\"][-1][\"train\"].X.map(len).sort_values())\n",
    "        index = round(len(tmp_sorted) * MAX_PEDDING_RATIO)\n",
    "        item[\"encodes\"][-1][\"train\"][\"X\"] = list(pad_sequences(item[\"encodes\"][-1][\"train\"][\"X\"].values, \n",
    "                                                               maxlen=tmp_sorted[index]))\n",
    "        item[\"encodes\"][-1][\"test\"][\"X\"] = list(pad_sequences(item[\"encodes\"][-1][\"test\"][\"X\"].values, \n",
    "                                                              maxlen=tmp_sorted[index]))\n",
    "        print(\"PADDED TRAIN AND TEST: \", tmp_sorted[index], vocab.name, item[\"db\"].name)                                 \n",
    "    # break"
   ]
  },
  {
   "source": [
    "# Train and Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Simple_IMDB_word_piece\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 694, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 5552)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                355392    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 363,457\n",
      "Trainable params: 363,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "500/500 [==============================] - 7s 14ms/step - loss: 0.5159 - binary_accuracy: 0.7248 - val_loss: 0.4000 - val_binary_accuracy: 0.8223\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 9s 17ms/step - loss: 0.3364 - binary_accuracy: 0.8546 - val_loss: 0.4245 - val_binary_accuracy: 0.8115\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2701 - binary_accuracy: 0.8886 - val_loss: 0.4764 - val_binary_accuracy: 0.8002\n",
      "Model: \"Simple_IMDB_word_piece\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 694, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 5552)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                355392    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 363,457\n",
      "Trainable params: 363,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "500/500 [==============================] - 7s 15ms/step - loss: 0.4941 - binary_accuracy: 0.7733 - val_loss: 0.3961 - val_binary_accuracy: 0.8216\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 6s 12ms/step - loss: 0.3307 - binary_accuracy: 0.8585 - val_loss: 0.4162 - val_binary_accuracy: 0.8151\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 6s 13ms/step - loss: 0.2652 - binary_accuracy: 0.8898 - val_loss: 0.4867 - val_binary_accuracy: 0.7945\n",
      "{'loss': [0.515944242477417, 0.3364292085170746, 0.2700512707233429], 'binary_accuracy': [0.724839985370636, 0.854640007019043, 0.8885999917984009], 'val_loss': [0.40002235770225525, 0.4244532883167267, 0.4764467477798462], 'val_binary_accuracy': [0.8223199844360352, 0.8114799857139587, 0.8002399802207947]}\n",
      "{'loss': [0.49407559633255005, 0.33065539598464966, 0.2651905417442322], 'binary_accuracy': [0.7732599973678589, 0.8584799766540527, 0.8898000121116638], 'val_loss': [0.39611414074897766, 0.4161621928215027, 0.4867490530014038], 'val_binary_accuracy': [0.8216000199317932, 0.8151199817657471, 0.7945200204849243]}\n",
      "Model: \"Simple_IMDB_word_level\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 412, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3296)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                211008    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 219,073\n",
      "Trainable params: 219,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.4674 - binary_accuracy: 0.7618 - val_loss: 0.3492 - val_binary_accuracy: 0.8481\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.3039 - binary_accuracy: 0.8723 - val_loss: 0.3612 - val_binary_accuracy: 0.8436\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2505 - binary_accuracy: 0.8959 - val_loss: 0.4151 - val_binary_accuracy: 0.8260\n",
      "Model: \"Simple_IMDB_word_level\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 412, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 3296)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                211008    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 219,073\n",
      "Trainable params: 219,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.4747 - binary_accuracy: 0.7904 - val_loss: 0.3842 - val_binary_accuracy: 0.8209\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.3073 - binary_accuracy: 0.8701 - val_loss: 0.3553 - val_binary_accuracy: 0.8456\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 4s 7ms/step - loss: 0.2565 - binary_accuracy: 0.8974 - val_loss: 0.3939 - val_binary_accuracy: 0.8324\n",
      "Epoch 4/1000\n",
      "500/500 [==============================] - 4s 8ms/step - loss: 0.2151 - binary_accuracy: 0.9134 - val_loss: 0.4373 - val_binary_accuracy: 0.8250\n",
      "{'loss': [0.46739494800567627, 0.303946316242218, 0.25053468346595764], 'binary_accuracy': [0.7617599964141846, 0.8723199963569641, 0.8958799839019775], 'val_loss': [0.349170982837677, 0.3612382709980011, 0.4150516092777252], 'val_binary_accuracy': [0.8481199741363525, 0.8435999751091003, 0.8260400295257568]}\n",
      "{'loss': [0.474700391292572, 0.3072528839111328, 0.25650185346603394, 0.2151152789592743], 'binary_accuracy': [0.7904000282287598, 0.8701199889183044, 0.8974000215530396, 0.9133599996566772], 'val_loss': [0.3841771185398102, 0.3553254008293152, 0.3938664495944977, 0.4372910261154175], 'val_binary_accuracy': [0.8208799958229065, 0.8456400036811829, 0.832360029220581, 0.8250399827957153]}\n",
      "Model: \"Simple_IMDB_unigram\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 720, 8)            8000      \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                368704    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 376,769\n",
      "Trainable params: 376,769\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "454/500 [==========================>...] - ETA: 0s - loss: 0.5200 - binary_accuracy: 0.7184"
     ]
    },
    {
     "output_type": "error",
     "ename": "InvalidArgumentError",
     "evalue": " indices[15,439] = 1000 is not in [0, 1000)\n\t [[node Simple_IMDB_unigram/embedding_4/embedding_lookup (defined at f:\\projects\\POC\\evaltok\\drivers\\models\\simple.py:79) ]] [Op:__inference_train_function_30448]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Simple_IMDB_unigram/embedding_4/embedding_lookup:\n Simple_IMDB_unigram/embedding_4/embedding_lookup/30219 (defined at C:\\Users\\opell.DESKTOP-UEQ8DPV\\anaconda3\\envs\\E01\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8c41fa93064f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                        test_y=np.array(item[\"encodes\"][i][\"test\"][\"y\"].values))\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m        \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m    \u001b[1;31m# break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mf:\\projects\\POC\\evaltok\\drivers\\models\\simple.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m                                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                                     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                                     validation_data=(self.test_X, self.test_y))\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  indices[15,439] = 1000 is not in [0, 1000)\n\t [[node Simple_IMDB_unigram/embedding_4/embedding_lookup (defined at f:\\projects\\POC\\evaltok\\drivers\\models\\simple.py:79) ]] [Op:__inference_train_function_30448]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node Simple_IMDB_unigram/embedding_4/embedding_lookup:\n Simple_IMDB_unigram/embedding_4/embedding_lookup/30219 (defined at C:\\Users\\opell.DESKTOP-UEQ8DPV\\anaconda3\\envs\\E01\\lib\\contextlib.py:112)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    " for item in dbs:\n",
    "    for i in range(len(item[\"vocabs\"])):\n",
    "        simple_name = \"Simple_\" + item[\"db\"].name + \"_\" + item[\"vocabs\"][i].name\n",
    "        simple = Simple(vocab_size=item[\"vocabs\"][i].vocab_size, \n",
    "                        input_lenght=len(item[\"encodes\"][i][\"train\"][\"X\"].values[0]), \n",
    "                        embedding_size=8,\n",
    "                        output_size=item[\"db\"].get_labels(),\n",
    "                        repeate=2,\n",
    "                        name=simple_name)\n",
    "        \n",
    "        simple.set_data(train_X=np.array([item for item in item[\"encodes\"][i][\"train\"][\"X\"].values]), \n",
    "                        train_y=np.array(item[\"encodes\"][i][\"train\"][\"y\"].values), \n",
    "                        test_X=np.array([item for item in item[\"encodes\"][i][\"test\"][\"X\"].values]), \n",
    "                        test_y=np.array(item[\"encodes\"][i][\"test\"][\"y\"].values))\n",
    "\n",
    "        history = simple.fit()\n",
    "        for h in history: print(h.history)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}