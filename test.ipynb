{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd071f835d09f2dc7658b8bfea70337fd38a098aa1e53e53fe645aaa17deb87e84b",
   "display_name": "Python 3.7.9 64-bit ('E01': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from drivers.loaders.imdb import IMDB\n",
    "from drivers.loaders.newsspace200 import Newsspace200\n",
    "from drivers.loaders.sentimentMD import SentimentMD\n",
    "from drivers.loaders.sentiment140 import Sentiment140\n",
    "\n",
    "from drivers.tokenizers.word_piece_vocab import WordPieceVocab\n",
    "from drivers.tokenizers.word_level_vocab import WordLevelVocab\n",
    "from drivers.tokenizers.unigram_vocab import UnigramVocab\n",
    "from drivers.tokenizers.bpe_vocab import BPEVocab\n",
    "\n",
    "from drivers.models.simple import Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "UNK_TOKEN = \"[UNK]\"\n",
    "PATH_VOCABS = \"vocabs/\"\n",
    "PATH_ENCODES = \"encodes/\"\n",
    "MAX_PEDDING_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(model, data):\n",
    "    result = []\n",
    "    for item in data:\n",
    "        output = model.encode(item)\n",
    "        result.append(output.ids)\n",
    "    return result"
   ]
  },
  {
   "source": [
    "# Data loading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "dbs = [\n",
    "    { \"db\" : SentimentMD(\"data\") },\n",
    "    { \"db\" : Sentiment140(\"data\") },\n",
    "    { \"db\" : Newsspace200(\"data\") },\n",
    "    { \"db\" : IMDB(\"data\") }\n",
    "]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "source": [
    "# Train tokenizers"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in dbs:\n",
    "    item[\"vocabs\"] = [\n",
    "        WordPieceVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        WordLevelVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        UnigramVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE),\n",
    "        BPEVocab(item[\"db\"].get_train()[\"text\"].values, UNK_TOKEN, VOCAB_SIZE)\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "vocabs/word_piece_SentimentMD.json\nLOADED: SentimentMD\nvocabs/word_level_SentimentMD.json\nLOADED: SentimentMD\nvocabs/unigram_SentimentMD.json\nLOADED: SentimentMD\nvocabs/bpe_SentimentMD.json\nLOADED: SentimentMD\nvocabs/word_piece_Sentiment140.json\nLOADED: Sentiment140\nvocabs/word_level_Sentiment140.json\nLOADED: Sentiment140\nvocabs/unigram_Sentiment140.json\nLOADED: Sentiment140\nvocabs/bpe_Sentiment140.json\nLOADED: Sentiment140\nvocabs/word_piece_Newsspace200.json\nLOADED: Newsspace200\nvocabs/word_level_Newsspace200.json\nLOADED: Newsspace200\nvocabs/unigram_Newsspace200.json\nLOADED: Newsspace200\nvocabs/bpe_Newsspace200.json\nLOADED: Newsspace200\nvocabs/word_piece_IMDB.json\nLOADED: IMDB\nvocabs/word_level_IMDB.json\nLOADED: IMDB\nvocabs/unigram_IMDB.json\nLOADED: IMDB\nvocabs/bpe_IMDB.json\nLOADED: IMDB\n"
     ]
    }
   ],
   "source": [
    "for item in dbs:\n",
    "    for vocab in item[\"vocabs\"]:\n",
    "        file_name_vocabs = PATH_VOCABS + vocab.name + \"_\" + item[\"db\"].name + \".json\"\n",
    "        print(file_name_vocabs)\n",
    "        if os.path.isfile(file_name_vocabs) == False:\n",
    "            vocab.train()\n",
    "            print(\"TRAINED:\", item[\"db\"].name)\n",
    "            vocab.save(file_name_vocabs)\n",
    "            print(\"SAVED:\", item[\"db\"].name)\n",
    "        else:\n",
    "            vocab.load(file_name_vocabs)\n",
    "            print(\"LOADED:\", item[\"db\"].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ENCODED (LOADED): encodes/word_piece_SentimentMD_train.json encodes/word_piece_SentimentMD_test.json word_piece SentimentMD\n",
      "PADDED TRAIN AND TEST:  307 word_piece SentimentMD\n",
      "ENCODED (LOADED): encodes/word_level_SentimentMD_train.json encodes/word_level_SentimentMD_test.json word_level SentimentMD\n",
      "PADDED TRAIN AND TEST:  184 word_level SentimentMD\n",
      "ENCODED (LOADED): encodes/unigram_SentimentMD_train.json encodes/unigram_SentimentMD_test.json unigram SentimentMD\n",
      "PADDED TRAIN AND TEST:  323 unigram SentimentMD\n",
      "ENCODED (LOADED): encodes/bpe_SentimentMD_train.json encodes/bpe_SentimentMD_test.json bpe SentimentMD\n",
      "PADDED TRAIN AND TEST:  287 bpe SentimentMD\n",
      "ENCODED (LOADED): encodes/word_piece_Sentiment140_train.json encodes/word_piece_Sentiment140_test.json word_piece Sentiment140\n",
      "PADDED TRAIN AND TEST:  41 word_piece Sentiment140\n",
      "ENCODED (LOADED): encodes/word_level_Sentiment140_train.json encodes/word_level_Sentiment140_test.json word_level Sentiment140\n",
      "PADDED TRAIN AND TEST:  25 word_level Sentiment140\n",
      "ENCODED (LOADED): encodes/unigram_Sentiment140_train.json encodes/unigram_Sentiment140_test.json unigram Sentiment140\n",
      "PADDED TRAIN AND TEST:  39 unigram Sentiment140\n",
      "ENCODED (LOADED): encodes/bpe_Sentiment140_train.json encodes/bpe_Sentiment140_test.json bpe Sentiment140\n",
      "PADDED TRAIN AND TEST:  38 bpe Sentiment140\n",
      "ENCODED (LOADED): encodes/word_piece_Newsspace200_train.json encodes/word_piece_Newsspace200_test.json word_piece Newsspace200\n",
      "PADDED TRAIN AND TEST:  87 word_piece Newsspace200\n",
      "ENCODED (LOADED): encodes/word_level_Newsspace200_train.json encodes/word_level_Newsspace200_test.json word_level Newsspace200\n",
      "PADDED TRAIN AND TEST:  44 word_level Newsspace200\n",
      "ENCODED (LOADED): encodes/unigram_Newsspace200_train.json encodes/unigram_Newsspace200_test.json unigram Newsspace200\n",
      "PADDED TRAIN AND TEST:  92 unigram Newsspace200\n",
      "ENCODED (LOADED): encodes/bpe_Newsspace200_train.json encodes/bpe_Newsspace200_test.json bpe Newsspace200\n",
      "PADDED TRAIN AND TEST:  82 bpe Newsspace200\n",
      "ENCODED (LOADED): encodes/word_piece_IMDB_train.json encodes/word_piece_IMDB_test.json word_piece IMDB\n",
      "PADDED TRAIN AND TEST:  694 word_piece IMDB\n",
      "ENCODED (LOADED): encodes/word_level_IMDB_train.json encodes/word_level_IMDB_test.json word_level IMDB\n",
      "PADDED TRAIN AND TEST:  412 word_level IMDB\n",
      "ENCODED (LOADED): encodes/unigram_IMDB_train.json encodes/unigram_IMDB_test.json unigram IMDB\n",
      "PADDED TRAIN AND TEST:  720 unigram IMDB\n",
      "ENCODED (LOADED): encodes/bpe_IMDB_train.json encodes/bpe_IMDB_test.json bpe IMDB\n",
      "PADDED TRAIN AND TEST:  650 bpe IMDB\n"
     ]
    }
   ],
   "source": [
    "for item in dbs:\n",
    "    item[\"encodes\"] = []\n",
    "    for vocab in item[\"vocabs\"]:\n",
    "        file_name_encodes_train = PATH_ENCODES + vocab.name + \"_\" + item[\"db\"].name + \"_train.json\"\n",
    "        file_name_encodes_test = PATH_ENCODES + vocab.name + \"_\" + item[\"db\"].name + \"_test.json\"\n",
    "        item[\"encodes\"].append({\n",
    "            \"train\" : pd.DataFrame(),\n",
    "            \"test\" : pd.DataFrame(),\n",
    "        })\n",
    "                       \n",
    "        if os.path.isfile(file_name_encodes_train) == False or os.path.isfile(file_name_encodes_test) == False:\n",
    "            item[\"encodes\"][-1][\"train\"][\"X\"] = encode(vocab, item[\"db\"].get_train()[\"text\"].values)\n",
    "            item[\"encodes\"][-1][\"test\"][\"X\"] = encode(vocab, item[\"db\"].get_test()[\"text\"].values)\n",
    "                        \n",
    "            if item[\"db\"].get_labels() > 1:\n",
    "                train_y = tf.one_hot(item[\"db\"].get_train()[\"label\"].values, item[\"db\"].get_labels()+1).numpy()\n",
    "                train_y = [list(item) for item in train_y]         \n",
    "                item[\"encodes\"][-1][\"train\"][\"y\"] = train_y\n",
    "\n",
    "                test_y = tf.one_hot(item[\"db\"].get_test()[\"label\"].values, item[\"db\"].get_labels()+1).numpy()      \n",
    "                test_y = [list(item) for item in test_y]\n",
    "                item[\"encodes\"][-1][\"test\"][\"y\"] = test_y\n",
    "            else:\n",
    "                item[\"encodes\"][-1][\"train\"][\"y\"] = item[\"db\"].get_train()[\"label\"].values                        \n",
    "                item[\"encodes\"][-1][\"test\"][\"y\"] = item[\"db\"].get_test()[\"label\"].values\n",
    "            \n",
    "            item[\"encodes\"][-1][\"train\"].to_json(file_name_encodes_train, orient=\"records\", lines=True)\n",
    "            item[\"encodes\"][-1][\"test\"].to_json(file_name_encodes_test, orient=\"records\", lines=True)\n",
    "\n",
    "            print(\"ENCODED (CREATED AND LOADED):\", \n",
    "                  file_name_encodes_train, \n",
    "                  file_name_encodes_test,\n",
    "                  vocab.name, \n",
    "                  item[\"db\"].name)\n",
    "        else:                        \n",
    "            item[\"encodes\"][-1][\"train\"] = pd.read_json(file_name_encodes_train, orient=\"records\", lines=True)\n",
    "            item[\"encodes\"][-1][\"test\"] = pd.read_json(file_name_encodes_test, orient=\"records\", lines=True)\n",
    "            print(\"ENCODED (LOADED):\", \n",
    "                  file_name_encodes_train, \n",
    "                  file_name_encodes_test, \n",
    "                  vocab.name, \n",
    "                  item[\"db\"].name)\n",
    "\n",
    "        tmp_sorted = list(item[\"encodes\"][-1][\"train\"].X.map(len).sort_values())\n",
    "        index = round(len(tmp_sorted) * MAX_PEDDING_RATIO)\n",
    "        item[\"encodes\"][-1][\"train\"][\"X\"] = list(pad_sequences(item[\"encodes\"][-1][\"train\"][\"X\"].values, \n",
    "                                                               maxlen=tmp_sorted[index]))\n",
    "        item[\"encodes\"][-1][\"test\"][\"X\"] = list(pad_sequences(item[\"encodes\"][-1][\"test\"][\"X\"].values, \n",
    "                                                              maxlen=tmp_sorted[index]))\n",
    "        print(\"PADDED TRAIN AND TEST: \", tmp_sorted[index], vocab.name, item[\"db\"].name)                                 \n",
    "    # break"
   ]
  },
  {
   "source": [
    "# Train and Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"Simple_IMDB_word_piece\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 694, 8)            8008      \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 5552)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 64)                355392    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 363,465\n",
      "Trainable params: 363,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "500/500 [==============================] - 11s 22ms/step - loss: 0.5107 - binary_accuracy: 0.7320 - auc_8: 0.8231 - precision_7: 0.7310 - recall_6: 0.7340 - true_positives_6: 9175.0000 - true_negatives_6: 9124.0000 - false_positives_5: 3376.0000 - false_negatives_5: 3325.0000 - sensitivity_at_specificity_3: 0.9106 - specificity_at_sensitivity_3: 0.9074 - val_loss: 0.4014 - val_binary_accuracy: 0.8178 - val_auc_8: 0.9024 - val_precision_7: 0.8467 - val_recall_6: 0.7762 - val_true_positives_6: 9702.0000 - val_true_negatives_6: 10743.0000 - val_false_positives_5: 1757.0000 - val_false_negatives_5: 2798.0000 - val_sensitivity_at_specificity_3: 0.9660 - val_specificity_at_sensitivity_3: 0.9634\n",
      "Epoch 2/1000\n",
      "500/500 [==============================] - 9s 18ms/step - loss: 0.3282 - binary_accuracy: 0.8599 - auc_8: 0.9342 - precision_7: 0.8604 - recall_6: 0.8593 - true_positives_6: 10741.0000 - true_negatives_6: 10757.0000 - false_positives_5: 1743.0000 - false_negatives_5: 1759.0000 - sensitivity_at_specificity_3: 0.9814 - specificity_at_sensitivity_3: 0.9820 - val_loss: 0.4353 - val_binary_accuracy: 0.8086 - val_auc_8: 0.8941 - val_precision_7: 0.7740 - val_recall_6: 0.8719 - val_true_positives_6: 10899.0000 - val_true_negatives_6: 9317.0000 - val_false_positives_5: 3183.0000 - val_false_negatives_5: 1601.0000 - val_sensitivity_at_specificity_3: 0.9602 - val_specificity_at_sensitivity_3: 0.9601\n",
      "Epoch 3/1000\n",
      "500/500 [==============================] - 8s 16ms/step - loss: 0.2647 - binary_accuracy: 0.8897 - auc_8: 0.9575 - precision_7: 0.8890 - recall_6: 0.8906 - true_positives_6: 11133.0000 - true_negatives_6: 11110.0000 - false_positives_5: 1390.0000 - false_negatives_5: 1367.0000 - sensitivity_at_specificity_3: 0.9923 - specificity_at_sensitivity_3: 0.9916 - val_loss: 0.4653 - val_binary_accuracy: 0.8016 - val_auc_8: 0.8828 - val_precision_7: 0.8008 - val_recall_6: 0.8029 - val_true_positives_6: 10036.0000 - val_true_negatives_6: 10004.0000 - val_false_positives_5: 2496.0000 - val_false_negatives_5: 2464.0000 - val_sensitivity_at_specificity_3: 0.9518 - val_specificity_at_sensitivity_3: 0.9509\n",
      "Model: \"Simple_IMDB_word_piece\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 694, 8)            8008      \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 5552)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 64)                355392    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 363,465\n",
      "Trainable params: 363,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "449/500 [=========================>....] - ETA: 0s - loss: 0.5263 - binary_accuracy: 0.7606 - auc_8: 0.8492 - precision_7: 0.7571 - recall_6: 0.7683 - true_positives_6: 18249.0000 - true_negatives_6: 17843.0000 - false_positives_5: 5855.0000 - false_negatives_5: 5503.0000 - sensitivity_at_specificity_3: 0.9333 - specificity_at_sensitivity_3: 0.9298"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e9d310a23aa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m                        test_y=np.array([item for item in item[\"encodes\"][i][\"test\"][\"y\"].values]))\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m        \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m        \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m    \u001b[1;31m# break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-4dc941dff010>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     90\u001b[0m                                      \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                                      \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                                      validation_data=(self.test_X, self.test_y))\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1098\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1099\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    805\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 807\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2829\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2831\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[0;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1922\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1924\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\E01\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " for item in dbs:\n",
    "    for i in range(len(item[\"vocabs\"])):\n",
    "        simple_name = \"Simple_\" + item[\"db\"].name + \"_\" + item[\"vocabs\"][i].name\n",
    "        simple = Simple2(vocab_size=item[\"vocabs\"][i].vocab_size, \n",
    "                        input_lenght=len(item[\"encodes\"][i][\"train\"][\"X\"].values[0]), \n",
    "                        embedding_size=8,\n",
    "                        output_size=item[\"db\"].get_labels(),\n",
    "                        repeate=2,\n",
    "                        name=simple_name)\n",
    "        \n",
    "        simple.set_data(train_X=np.array([item for item in item[\"encodes\"][i][\"train\"][\"X\"].values]), \n",
    "                        train_y=np.array([item for item in item[\"encodes\"][i][\"train\"][\"y\"].values]), \n",
    "                        test_X=np.array([item for item in item[\"encodes\"][i][\"test\"][\"X\"].values]), \n",
    "                        test_y=np.array([item for item in item[\"encodes\"][i][\"test\"][\"y\"].values]))\n",
    "\n",
    "        history = simple.fit()\n",
    "        for h in history: print(h.history)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}